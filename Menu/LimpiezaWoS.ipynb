{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4909769",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677cc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6830e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leelos los archivos CSV\n",
    "nombresConId = pd.read_csv('../inventores_con_id.csv')\n",
    "inventoresPatentes = pd.read_csv('../InventoresPatentes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d0efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quitamos acentos a los nombres de los inventores en nombresConId en la columna 'Inventor'\n",
    "nombresConId['Inventor'] = nombresConId['Inventor'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953e7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cruzamos los dos dataframes por la columna 'Inventor' y 'Inventor' y solamente nos quedamos con la columna inventor_id del df nombresConId\n",
    "inventoresPatentes1 = pd.merge(inventoresPatentes, nombresConId[['Inventor', 'inventor_id']], on='Inventor', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcce1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos duplicados conforme al inventores y patentes\n",
    "inventoresPatentes1 = inventoresPatentes1.drop_duplicates(subset=['Inventor', 'inventor_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd296358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el resultado en un nuevo archivo CSV\n",
    "inventoresPatentes1.to_csv('../datasetPatentes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ef00d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d7963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenados 190 archivos en 'dataset_autores_wos.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "# Ruta de la carpeta 'Autores WoS'\n",
    "carpeta = '../Autores WoS'\n",
    "\n",
    "# Lista para almacenar los DataFrames procesados\n",
    "dataframes = []\n",
    "\n",
    "# Mapa de meses\n",
    "MESES = {\n",
    "    'JAN':1, 'FEB':2, 'MAR':3, 'APR':4, 'MAY':5, 'JUN':6,\n",
    "    'JUL':7, 'AUG':8, 'SEP':9, 'OCT':10, 'NOV':11, 'DEC':12\n",
    "}\n",
    "\n",
    "# Definir constantes\n",
    "SIN_DATOS = 'Sin datos'\n",
    "TOTAL_CITAS = 'Total de Citas'\n",
    "PROMEDIO_POR_AÑO = 'Promedio por año'\n",
    "PIBLICACION_DATE= 'Publication Date'\n",
    "\n",
    "def calcular_indice_h(df):\n",
    "    \"\"\"\n",
    "    Calcula el índice h para un DataFrame basado en la columna TOTAL_CITAS.\n",
    "    \"\"\"\n",
    "    citas = df[TOTAL_CITAS].sort_values(ascending=False).values\n",
    "    h_index = 0\n",
    "    for i, c in enumerate(citas):\n",
    "        if c >= i + 1:\n",
    "            h_index = i + 1\n",
    "        else:\n",
    "            break\n",
    "    return h_index\n",
    "\n",
    "def normaliza_fecha(s):\n",
    "    if pd.isna(s):\n",
    "        return pd.NaT\n",
    "\n",
    "    txt_original = str(s)\n",
    "    txt = txt_original.strip().upper()\n",
    "    txt = re.sub(r'\\s+', ' ', txt)  # Normaliza espacios\n",
    "    txt = re.sub(r'^(SPR|SUM|FAL|WIN)(\\d{4})$', r'\\1 \\2', txt)  # WIN1999 → WIN 1999\n",
    "    txt = re.sub(r'^([A-Z]{3})-[A-Z]{3} (\\d{4})$', r'\\1 \\2', txt)  # APR-JUN 2008 → APR 2008\n",
    "\n",
    "    season_months = {'SPR': 3, 'SUM': 6, 'FAL': 9, 'WIN': 12}\n",
    "    MESES = {\n",
    "        'JAN': 1, 'FEB': 2, 'MAR': 3, 'APR': 4, 'MAY': 5, 'JUN': 6,\n",
    "        'JUL': 7, 'AUG': 8, 'SEP': 9, 'OCT': 10, 'NOV': 11, 'DEC': 12\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if re.fullmatch(r'\\d{4}', txt):\n",
    "            return pd.Timestamp(int(txt), 1, 1)\n",
    "\n",
    "        # Primero manejar temporadas\n",
    "        m3 = re.fullmatch(r'(SPR|SUM|FAL|WIN) (\\d{4})', txt)\n",
    "        if m3:\n",
    "            season, anio = m3.groups()\n",
    "            return pd.Timestamp(int(anio), season_months[season], 1)\n",
    "\n",
    "        # A continuación años abreviados\n",
    "        m = re.fullmatch(r'([A-Z]{3})-(\\d{2})', txt)\n",
    "        if m:\n",
    "            mon, yy = m.groups()\n",
    "            return pd.Timestamp(2000 + int(yy), MESES[mon], 1)\n",
    "\n",
    "        # Solo procesar meses válidos\n",
    "        m2 = re.fullmatch(r'([A-Z]{3}) (\\d{4})', txt)\n",
    "        if m2:\n",
    "            mon, anio = m2.groups()\n",
    "            if mon in MESES:\n",
    "                return pd.Timestamp(int(anio), MESES[mon], 1)\n",
    "\n",
    "        # Si es solo temporada sin año\n",
    "        if txt in season_months:\n",
    "            hoy = pd.Timestamp.now()\n",
    "            return pd.Timestamp(hoy.year, season_months[txt], 1)\n",
    "\n",
    "        return pd.to_datetime(txt, errors='raise')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Fecha no convertida: '{txt_original}' (procesado como: '{txt}') → {e}\")\n",
    "        return pd.NaT\n",
    "\n",
    "for archivo in os.listdir(carpeta):\n",
    "    if archivo.endswith('.txt'):\n",
    "        try:\n",
    "            # Leer el archivo y eliminar las primeras 3 filas\n",
    "            df = pd.read_csv(\n",
    "                os.path.join(carpeta, archivo),\n",
    "                skiprows=3,\n",
    "                on_bad_lines='skip',\n",
    "                engine='python'\n",
    "            )\n",
    "\n",
    "            # Agregar la columna \"Investigador\"\n",
    "            df['Investigador'] = os.path.splitext(archivo)[0]\n",
    "\n",
    "            # contamos cuantos autores hay en la columna 'Authors' separados po ;\n",
    "            df['Num_Autores'] = df['Authors'].str.split(';').str.len()\n",
    "\n",
    "            # Normalizamos la columna 'Publicarion Date'\n",
    "            df[PIBLICACION_DATE] = df[PIBLICACION_DATE]\\\n",
    "                .apply(normaliza_fecha)\\\n",
    "                .dt.date\n",
    "            \n",
    "            # Si PIBLICACION_DATE es NA vemos que año hay en 'Publication Year' y lo pasamos a fecha en formato YYYY-MM-DD\n",
    "            df[PIBLICACION_DATE] = df.apply(\n",
    "                lambda x: pd.Timestamp(f\"{x['Publication Year']}-01-01\").date() \n",
    "                if pd.isna(x[PIBLICACION_DATE]) else x[PIBLICACION_DATE], \n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Eliminar sólo las columnas no deseadas\n",
    "            df = df.drop(columns=['Average per Year', 'Total Citations'], errors='ignore')\n",
    "\n",
    "            # Detectar columnas de años 2000–2024\n",
    "            columnas_años = [str(año) for año in range(2000, 2024)]\n",
    "            años_existentes = [c for c in columnas_años if c in df.columns]\n",
    "\n",
    "            # Eliminar columnas de años 1900–1999 y la del 2025\n",
    "            columnas_a_eliminar = [str(año) for año in range(1900, 2000)]\n",
    "            df = df.drop(columns=columnas_a_eliminar, errors='ignore')\n",
    "            df = df.drop(columns=['2025'], errors='ignore')\n",
    "\n",
    "            df[TOTAL_CITAS] = df[años_existentes].sum(axis=1)\n",
    "            df[PROMEDIO_POR_AÑO] = df['Total de Citas'] / ((2024 - df['Publication Year']) + 1)\n",
    "\n",
    "            # 'Promedio por año' lo dejamos a 2 digitos\n",
    "            df[PROMEDIO_POR_AÑO] = df[PROMEDIO_POR_AÑO].round(2)\n",
    "\n",
    "            # Calcular el índice h para el investigador\n",
    "            df['h'] = calcular_indice_h(df)\n",
    "\n",
    "            dataframes.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar {archivo}: {e}\")\n",
    "\n",
    "# Concatenar y exportar\n",
    "df_combinado = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "df_combinado[TOTAL_CITAS] = df_combinado[TOTAL_CITAS].fillna(0)\n",
    "df_combinado[PROMEDIO_POR_AÑO] = df_combinado[PROMEDIO_POR_AÑO].fillna(0)\n",
    "df_combinado.replace([float('inf'), float('-inf')], float('nan'), inplace=True)\n",
    "\n",
    "# En estas columnas 'Title', 'Investigador', 'Corporate Authors', 'Book Editors', 'Source Title' si hay NaN los convertimos a 'Sin datos'\n",
    "df_combinado['Title'] = df_combinado['Title'].fillna(SIN_DATOS)\n",
    "df_combinado['Investigador'] = df_combinado['Investigador'].fillna(SIN_DATOS)\n",
    "df_combinado['Corporate Authors'] = df_combinado['Corporate Authors'].fillna(SIN_DATOS)\n",
    "df_combinado['Book Editors'] = df_combinado['Book Editors'].fillna(SIN_DATOS)\n",
    "df_combinado['Source Title'] = df_combinado['Source Title'].fillna(SIN_DATOS)\n",
    "\n",
    "# Guardamos el resultado en un nuevo archivo CSV\n",
    "df_combinado.to_csv('../Analisis/datasetWoS.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Concatenados {len(dataframes)} archivos en 'dataset_autores_wos.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ed766ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title\n",
      "Authors\n",
      "Corporate Authors\n",
      "Editors\n",
      "Book Editors\n",
      "Source Title\n",
      "Publication Date\n",
      "Publication Year\n",
      "Volume\n",
      "Issue\n",
      "Part Number\n",
      "Supplement\n",
      "Special Issue\n",
      "Beginning Page\n",
      "Ending Page\n",
      "Article Number\n",
      "DOI\n",
      "Conference Title\n",
      "Conference Date\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "Investigador\n",
      "Num_Autores\n",
      "Total de Citas\n",
      "Promedio por año\n",
      "h\n"
     ]
    }
   ],
   "source": [
    "# Enumerar las columnas con sus índices\n",
    "for i, col in enumerate(df_combinado.columns):\n",
    "    print(f\"{col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "134e8a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay valores NaN en el DataFrame.\n",
      "Title                   0\n",
      "Authors                 0\n",
      "Corporate Authors       0\n",
      "Editors              4063\n",
      "Book Editors            0\n",
      "Source Title            0\n",
      "Publication Date       16\n",
      "Publication Year        0\n",
      "Volume                286\n",
      "Issue                1265\n",
      "Part Number          3979\n",
      "Supplement           3968\n",
      "Special Issue        3919\n",
      "Beginning Page        773\n",
      "Ending Page           773\n",
      "Article Number       3351\n",
      "DOI                   559\n",
      "Conference Title     3442\n",
      "Conference Date      3442\n",
      "2000                    0\n",
      "2001                    0\n",
      "2002                    0\n",
      "2003                    0\n",
      "2004                    0\n",
      "2005                    0\n",
      "2006                    0\n",
      "2007                    0\n",
      "2008                    0\n",
      "2009                    0\n",
      "2010                    0\n",
      "2011                    0\n",
      "2012                    0\n",
      "2013                    0\n",
      "2014                    0\n",
      "2015                    0\n",
      "2016                    0\n",
      "2017                    0\n",
      "2018                    0\n",
      "2019                    0\n",
      "2020                    0\n",
      "2021                    0\n",
      "2022                    0\n",
      "2023                    0\n",
      "2024                    0\n",
      "2025                    0\n",
      "Investigador            0\n",
      "Num_Autores             0\n",
      "Total de Citas          0\n",
      "Promedio por año        1\n",
      "h                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay valores NaN en las columnas\n",
    "if df_combinado.isnull().any().any():\n",
    "    print(\"Hay valores NaN en el DataFrame.\")\n",
    "    print(df_combinado.isnull().sum())  # Mostrar el conteo de NaN por columna\n",
    "\n",
    "# Verificar si hay valores infinitos en las columnas\n",
    "if (df_combinado == float('inf')).any().any() or (df_combinado == float('-inf')).any().any():\n",
    "    print(\"Hay valores infinitos en el DataFrame.\")\n",
    "    print(df_combinado[(df_combinado == float('inf')) | (df_combinado == float('-inf'))])  # Mostrar las filas con valores infinitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03efbe61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carpeta '../Autores WoS Limpios' se creó correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Leemos cada txt de la carpeta 'Autores WoS' y los guardamos en una nueva carpeta llamada 'Autores WoS Limpios' donde solamente se le quiten las 3 primeras filas\n",
    "carpeta_limpia = '../Autores WoS Limpios'\n",
    "os.makedirs(carpeta_limpia, exist_ok=True)\n",
    "for archivo in os.listdir(carpeta):\n",
    "    if archivo.endswith('.txt'):\n",
    "        try:\n",
    "            # Leer el archivo y eliminar las primeras 3 filas\n",
    "            df = pd.read_csv(\n",
    "                os.path.join(carpeta, archivo),\n",
    "                skiprows=3,\n",
    "                on_bad_lines='skip',\n",
    "                engine='python'\n",
    "            )\n",
    "\n",
    "            # Guardar el DataFrame limpio en la nueva carpeta\n",
    "            df.to_csv(os.path.join(carpeta_limpia, archivo), index=False, encoding='utf-8')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar {archivo}: {e}\")\n",
    "\n",
    "# Verificar si la carpeta 'Autores WoS Limpios' se creó correctamente\n",
    "if os.path.exists(carpeta_limpia):\n",
    "    print(f\"La carpeta '{carpeta_limpia}' se creó correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254662f6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "735249b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../Analisis/Nombres_PxS.csv')\n",
    "\n",
    "# Hacemos que la columna 'NOMBRE SNII' tenga formato de 'NOMBRE DEL INVESTIGADOR'\n",
    "df['NOMBRE SNII'] = df['NOMBRE DEL INVESTIGADOR'].str.replace(',', ' ', regex=False)\n",
    "\n",
    "# Eliminamos la columna Ratio, NOMBRE PATENTE\n",
    "df = df.drop(columns=['Ratio', 'NOMBRE PATENTE'], errors='ignore')\n",
    "\n",
    "# guardamos el resultado en un nuevo archivo CSV\n",
    "df.to_csv('../Analisis/Nombres_PxS.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT_entorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
